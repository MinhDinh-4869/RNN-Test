{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAZcFWNaZ2x7ybESuhEYGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinhDinh-4869/RNN-Test/blob/main/rnn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKotW21aoY_8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        " \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create an dictionary for one hot vector\n",
        "text = ['hey how are you ?','good i am fine','have a nice day', 'hello mother fucker', 'how are you today ?', 'i am good', 'i love you', 'my name is', 'what your name ?', 'what are you doing ?']\n",
        "\n",
        "#join all the senteces togenther and extract the unique characters from the \n",
        "#combined sentences chars\n",
        "\n",
        "chars = set(''.join(text))\n",
        "\n",
        "#creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "#creating a dictionary that maps characters to the integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "print(char2int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpAiXPNPokRT",
        "outputId": "c7392f30-3ee4-483a-d0f4-25d30234b0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'k': 0, 'a': 1, 'r': 2, 'i': 3, 'o': 4, ' ': 5, 't': 6, 'c': 7, 'h': 8, '?': 9, 'd': 10, 'e': 11, 'y': 12, 'g': 13, 'm': 14, 'f': 15, 'v': 16, 's': 17, 'n': 18, 'w': 19, 'l': 20, 'u': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the length of the longest string in our data\n",
        "maxlen = len(max(text, key = len))\n",
        "\n",
        "#padding\n",
        "\n",
        "#A simple loop that loops through the list of sentences and adds a ' ' white\n",
        "#space until the length of the sentences matches the length of the longest sentece\n",
        "\n",
        "for i in range(len(text)):\n",
        "  while(len(text[i]) < maxlen):\n",
        "    text[i]+= ' '\n"
      ],
      "metadata": {
        "id": "tzQRjTm9qpe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are going to predict the next character in the sequece at each time step, we will have to dived each sentence into\n",
        "#1. Input data: the last input charactyer should be excluded as it does not need to be fed into the model\n",
        "#2. target/ground truth label\n",
        "#e.g: in: hey how are yo --> ey how are you\n",
        "# input a word and predict the next word, and the size of the input sentence is fixed\n",
        "#as the network size is fixed. then the first character of the ground truth is missing\n",
        "\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "  #remove the last character\n",
        "  input_seq.append(text[i][:-1]) #[-1] is the last item in a list\n",
        "\n",
        "  #remove the first character\n",
        "  target_seq.append(text[i][1:])\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0PSrp2rquOm",
        "outputId": "74ec8460-588c-494f-b6db-9367e47c6fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to integer vectors\n",
        "print(input_seq)\n",
        "\n",
        "for i in range(len(text)):\n",
        "  input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "  target_seq[i] = [char2int[character0] for character0 in target_seq[i]]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L9gM7EYt9jP",
        "outputId": "d8426294-19b6-418a-ef2b-d011ce9d0da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey how are you ?  ', 'good i am fine     ', 'have a nice day    ', 'hello mother fucker', 'how are you today ?', 'i am good          ', 'i love you         ', 'my name is         ', 'what your name ?   ', 'what are you doing ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1 # length of a processed sequence = len - 1\n",
        "batch_size = len(text) # the number of sentences that we defined and are going to be fed into the model as a batch\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size): #for 1 sequence: target or input\n",
        "  #creating a multi dimensial array of zeros with the desired output shape\n",
        "  features  = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "\n",
        "  #replacing the 0 that the relevant character\n",
        "  for i in range(batch_size):\n",
        "    for u in range(seq_len):             #   i   u\n",
        "      features[i, u, sequence[i][u]] = 1 #e.g[[4,16,3,9,4,8,2,9,15,6,16,9,3,8]] = [[h,e,y,h,o,w,a,r,e,y,o]]\n",
        "  return features\n",
        "\n",
        "print(one_hot_encode(input_seq, dict_size, seq_len, batch_size))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOvCYnmcvmTs",
        "outputId": "97202881-60ff-4188-ec6f-6afec25853a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "#target_seq = one_hot_encode(target_seq, dict_size, seq_len, batch_size) !! dont need this\n",
        "\n",
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ],
      "metadata": {
        "id": "kjosGxSjKPvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJCIeqFHSxvZ",
        "outputId": "64ce0d3a-d934-4134-d78f-de86bb3e45a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOB5BvJMMDjF",
        "outputId": "ebe2625b-aa35-4954-c3be-3fe306e6f936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        #out = out.reshape(x.shape)\n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "gZAA0JUh3bwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 300\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "_fTIYPU5DU8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    #loss = criterion(output, target_seq)#).view(-1).long())\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    #output shape: (batch_size x input_size, dict_size) --> (42, 17)\n",
        "    #target shape: (batch_size, input_size) -> (3, 14)\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn8hHEv8FFwC",
        "outputId": "dfda1261-f060-4bef-c71b-2985da8b16f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/300............. Loss: 2.3489\n",
            "Epoch: 20/300............. Loss: 2.2215\n",
            "Epoch: 30/300............. Loss: 2.0669\n",
            "Epoch: 40/300............. Loss: 1.8596\n",
            "Epoch: 50/300............. Loss: 1.6099\n",
            "Epoch: 60/300............. Loss: 1.3811\n",
            "Epoch: 70/300............. Loss: 1.1876\n",
            "Epoch: 80/300............. Loss: 1.0202\n",
            "Epoch: 90/300............. Loss: 0.8638\n",
            "Epoch: 100/300............. Loss: 0.7351\n",
            "Epoch: 110/300............. Loss: 0.6275\n",
            "Epoch: 120/300............. Loss: 0.5378\n",
            "Epoch: 130/300............. Loss: 0.4621\n",
            "Epoch: 140/300............. Loss: 0.3978\n",
            "Epoch: 150/300............. Loss: 0.3471\n",
            "Epoch: 160/300............. Loss: 0.3078\n",
            "Epoch: 170/300............. Loss: 0.2727\n",
            "Epoch: 180/300............. Loss: 0.2455\n",
            "Epoch: 190/300............. Loss: 0.2224\n",
            "Epoch: 200/300............. Loss: 0.2028\n",
            "Epoch: 210/300............. Loss: 0.1977\n",
            "Epoch: 220/300............. Loss: 0.1856\n",
            "Epoch: 230/300............. Loss: 0.1664\n",
            "Epoch: 240/300............. Loss: 0.1546\n",
            "Epoch: 250/300............. Loss: 0.1447\n",
            "Epoch: 260/300............. Loss: 0.1363\n",
            "Epoch: 270/300............. Loss: 0.1289\n",
            "Epoch: 280/300............. Loss: 0.1223\n",
            "Epoch: 290/300............. Loss: 0.1159\n",
            "Epoch: 300/300............. Loss: 0.1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.functional import Tensor\n",
        "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
        "def test_predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    return prob\n",
        "\n",
        "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ],
      "metadata": {
        "id": "TsCnODJyMH0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob = test_predict(model, 'good did')\n",
        "for i in range(prob.shape[0]):\n",
        "  char_ind = np.argmax(prob.numpy()[i])\n",
        "  print(int2char[char_ind], np.max(prob.numpy()[i]))\n",
        "\n",
        "\n",
        "#print(int2char[char_ind])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNUmZtuBQTMw",
        "outputId": "fc687e0b-9b5b-404c-c9c5-8d4fe07e6859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k 4.759894e-05\n",
            "k 0.23260434\n",
            "k 0.00020718614\n",
            "k 9.54858e-05\n",
            "k 0.0008086724\n",
            "k 0.0023159874\n",
            "k 5.7562293e-06\n",
            "k 0.00015983757\n",
            "k 1.165815e-05\n",
            "k 0.011992644\n",
            "k 2.4871073e-05\n",
            "k 0.65938413\n",
            "k 0.066552036\n",
            "k 0.013119103\n",
            "k 6.2544e-05\n",
            "k 0.00096558844\n",
            "k 0.000117105614\n",
            "k 0.0007169355\n",
            "k 0.0048768814\n",
            "k 0.00097007054\n",
            "k 0.004290256\n",
            "k 0.00067125907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "164OrbHsML9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model, 20, 'what do')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5nBlR7oRMOA0",
        "outputId": "8eed0768-4a91-41fd-bc0f-d4fd22723607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what doy am good ing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o5eyEB0nvkTb"
      }
    }
  ]
}